# Processing Metrics Database - COMPLETE âœ…

**Date:** November 11, 2025

## What Was Built

A comprehensive metrics tracking system that stores quality checks in a dedicated database table, tagged by processing phase (Phase 1, 2, 3).

---

## Database Schema

### Table: `processing_metrics`

Stores quality metrics for each episode processing run:

```sql
CREATE TABLE processing_metrics (
    -- Identification
    id VARCHAR PRIMARY KEY,
    episode_id VARCHAR REFERENCES episodes(id),
    phase VARCHAR NOT NULL,              -- 'phase_1', 'phase_2', 'phase_3'
    processing_date TIMESTAMP,

    -- Transcript Quality Metrics (from Phase 1 checks)
    total_segments INTEGER,
    character_count INTEGER,
    word_count INTEGER,
    start_time FLOAT,
    end_time FLOAT,
    duration_covered_seconds FLOAT,
    gaps_detected INTEGER,
    is_complete BOOLEAN,

    -- Claude Processing Metrics (from Phase 1 checks)
    total_chunks INTEGER,
    total_characters_sent INTEGER,
    first_chunk_position INTEGER,
    last_chunk_position INTEGER,
    coverage_verified BOOLEAN,

    -- Recommendation Metrics
    total_recommendations_found INTEGER,
    unique_recommendations INTEGER,
    books_found INTEGER,
    movies_found INTEGER,
    products_found INTEGER,

    -- Cost & Performance
    ai_model_used VARCHAR,
    estimated_cost FLOAT,                -- USD
    processing_time_seconds FLOAT,

    -- Quality Flags
    had_errors BOOLEAN,
    error_message VARCHAR
);
```

---

## What Gets Stored

### For Each Episode Processing Run:

**Phase Information:**
- Which phase (Phase 1, 2, or 3)
- Processing timestamp

**Transcript Quality Checks:**
- âœ“ Total segments captured
- âœ“ Character count
- âœ“ Word count
- âœ“ Duration covered
- âœ“ Gaps detected
- âœ“ Is complete (boolean)

**Claude Processing Checks:**
- âœ“ Total chunks processed
- âœ“ Characters sent to Claude
- âœ“ First/last chunk positions
- âœ“ Coverage verified (boolean)

**Results:**
- âœ“ Total recommendations found
- âœ“ Unique after deduplication
- âœ“ Books/movies/products breakdown

**Performance:**
- âœ“ AI model used
- âœ“ Estimated cost
- âœ“ Processing time

---

## How It Works

### 1. Processing with Metrics

When you process an episode, metrics are automatically saved:

```python
# In process_all_pending.py
metrics_service.save_processing_metrics(
    db=db,
    episode_id=episode.id,
    phase="phase_1",  # or phase_2, phase_3
    transcript_metadata=transcript_metadata,
    claude_metadata=claude_metadata,
    recommendations=recommendations,
    ai_model="claude-sonnet-4-20250514",
    estimated_cost=0.0247,
    processing_time=142.5,
    had_errors=False,
    error_message=None
)
```

### 2. View Phase Summary

```bash
python scripts/view_phase_metrics.py --phase phase_1
```

**Output:**
```
================================================================================
  PHASE_1 SUMMARY
================================================================================

ğŸ“Š Episodes Processed: 10

ğŸ’° Cost Metrics:
  Total cost: $2.47
  Avg cost per episode: $0.0247

â±ï¸  Performance Metrics:
  Avg processing time: 142.5s (2.4 min)

ğŸ“š Recommendation Metrics:
  Total recommendations: 87
  Avg per episode: 8.7

âœ… Quality Metrics:
  Complete transcripts: 10/10 (100.0%)
  Episodes with errors: 0/10 (0.0%)
```

### 3. Compare Phases

```bash
python scripts/view_phase_metrics.py --compare phase_1 phase_2
```

**Output:**
```
================================================================================
  COMPARISON: PHASE_1 vs PHASE_2
================================================================================

ğŸ’° Cost Comparison:
  phase_1: $0.0247 per episode
  phase_2: $0.0012 per episode
  Savings: $0.0235 per episode (95.1%)
  âœ… phase_2 is 95.1% cheaper!

â±ï¸  Performance Comparison:
  phase_1: 142.5s
  phase_2: 135.2s
  Difference: 7.3s
  âœ… phase_2 is 7.3s faster!

ğŸ“š Quality Comparison:
  phase_1: 8.7 recommendations per episode
  phase_2: 8.2 recommendations per episode
  Difference: -0.5 (-5.7%)
  âœ… Quality maintained (within 10%)
```

---

## Available Commands

### List All Metrics
```bash
python scripts/view_phase_metrics.py --list
```

Shows recent processing runs with phase, cost, time, etc.

### View Phase Summary
```bash
python scripts/view_phase_metrics.py --phase phase_1
```

Shows aggregated stats for a phase.

### Compare Two Phases
```bash
python scripts/view_phase_metrics.py --compare phase_1 phase_2
```

Compares cost, performance, and quality between phases.

### View All Phases
```bash
python scripts/view_phase_metrics.py
```

Shows summary for all three phases.

---

## Query Metrics Programmatically

```python
from app.database import SessionLocal
from app.services.metrics_service import MetricsService
from app.models.processing_metrics import ProcessingMetrics

db = SessionLocal()

# Get all metrics for an episode
metrics = MetricsService.get_metrics_for_episode(db, episode_id)

# Get all metrics for a phase
phase1_metrics = MetricsService.get_metrics_by_phase(db, "phase_1")

# Get phase summary
summary = MetricsService.get_phase_summary(db, "phase_1")
print(f"Average cost: ${summary['avg_cost_per_episode']}")

# Compare phases
comparison = MetricsService.compare_phases(db, "phase_1", "phase_2")
print(f"Savings: {comparison['cost_comparison']['savings_percentage']}%")

# Query with SQLAlchemy
metrics = db.query(ProcessingMetrics).filter(
    ProcessingMetrics.phase == "phase_1",
    ProcessingMetrics.is_complete == True
).all()

for m in metrics:
    print(f"Episode: {m.episode_id}")
    print(f"  Segments: {m.total_segments}")
    print(f"  Characters: {m.character_count:,}")
    print(f"  Cost: ${m.estimated_cost:.4f}")
```

---

## Use Cases

### 1. Track Quality Over Time
```python
# Find episodes with incomplete transcripts
incomplete = db.query(ProcessingMetrics).filter(
    ProcessingMetrics.is_complete == False
).all()
```

### 2. Calculate Total Costs
```python
# Total spent in Phase 1
phase1 = db.query(ProcessingMetrics).filter(
    ProcessingMetrics.phase == "phase_1"
).all()

total_cost = sum(m.estimated_cost for m in phase1)
print(f"Phase 1 total cost: ${total_cost:.2f}")
```

### 3. Performance Analysis
```python
# Find slow processing runs
slow = db.query(ProcessingMetrics).filter(
    ProcessingMetrics.processing_time_seconds > 300  # > 5 minutes
).all()
```

### 4. A/B Testing Phases
```python
# Compare Phase 1 vs Phase 2 quality
comparison = MetricsService.compare_phases(db, "phase_1", "phase_2")

if comparison['quality_comparison']['difference_percentage'] > -10:
    print("âœ… Phase 2 maintains quality!")
else:
    print("âš ï¸ Phase 2 has quality issues")
```

---

## Files Created

1. âœ… `app/models/processing_metrics.py` - Database model
2. âœ… `app/services/metrics_service.py` - Service for saving/querying metrics
3. âœ… `scripts/migrate_add_processing_metrics.py` - Database migration
4. âœ… `scripts/view_phase_metrics.py` - CLI tool to view metrics
5. âœ… Updated `scripts/process_all_pending.py` - Saves metrics automatically

---

## Integration

### When Processing Episodes

Metrics are **automatically saved** when you process episodes:

```bash
python scripts/process_all_pending.py --limit 1
```

The script now:
1. Processes the episode
2. Captures all quality checks
3. **Saves to `processing_metrics` table**
4. Logs cost and time

### Current vs New Storage

**Before:**
- âœ… Logs only (console output)
- âœ… JSON fields in `episodes.transcript_metadata` and `episodes.claude_processing_metadata`

**Now (Added):**
- âœ… **Dedicated `processing_metrics` table**
- âœ… **Tagged by phase** (phase_1, phase_2, phase_3)
- âœ… **Queryable and comparable**
- âœ… **Aggregated statistics**

**All three storage methods are active!**

---

## Example: Full Workflow

### Step 1: Process Episode in Phase 1
```bash
python scripts/process_all_pending.py --limit 1
```

**Result:** Metrics saved with `phase="phase_1"`

### Step 2: View Phase 1 Stats
```bash
python scripts/view_phase_metrics.py --phase phase_1
```

**Output:**
```
ğŸ“Š Episodes Processed: 1
ğŸ’° Avg cost per episode: $0.0247
ğŸ“š Avg recommendations: 8
âœ… Complete transcripts: 100%
```

### Step 3: Implement Phase 2 (Cheaper AI)
```bash
# Switch to Gemini 1.5 Flash in code
# Process same episode
python scripts/process_all_pending.py --episode-id <id>
```

**Result:** Metrics saved with `phase="phase_2"`

### Step 4: Compare Results
```bash
python scripts/view_phase_metrics.py --compare phase_1 phase_2
```

**Output:**
```
ğŸ’° Savings: $0.0235 per episode (95.1%)
ğŸ“š Quality difference: -0.5 recommendations (-5.7%)
âœ… Quality maintained (within 10%)
```

---

## Benefits

### 1. **Data-Driven Decisions**
- Compare phases with real numbers
- See exact cost savings
- Track quality changes

### 2. **Historical Tracking**
- See metrics over time
- Identify quality trends
- Monitor error rates

### 3. **Cost Optimization**
- Calculate exact savings from Phase 2 vs Phase 1
- Project costs for 1000 episodes
- Justify infrastructure changes

### 4. **Quality Assurance**
- Track transcript completeness rates
- Monitor recommendation counts
- Flag processing errors

### 5. **Phase Comparison**
- A/B test different AI models
- Compare processing strategies
- Find optimal cost/quality balance

---

## Summary

âœ… **Database table created** - `processing_metrics`
âœ… **Metrics service implemented** - Save and query metrics
âœ… **Processing script updated** - Automatically saves metrics
âœ… **CLI tool created** - View and compare phases
âœ… **Phase tagging enabled** - Track Phase 1, 2, 3 separately

**Instead of just logging, all Phase 1 quality checks are now stored in a queryable database, tagged by phase!**

---

## Next Steps

1. Process episodes in Phase 1
2. View metrics with `view_phase_metrics.py`
3. Implement Phase 2 (cheaper AI)
4. Compare Phase 1 vs Phase 2 results
5. Make data-driven decision on which phase to use

---

**Status: READY TO USE** âœ…

All quality checks from Phase 1 are now stored in the database with phase tagging!
